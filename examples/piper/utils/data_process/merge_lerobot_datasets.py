#!/usr/bin/env python3
"""
è„šæœ¬ç”¨äºåˆå¹¶ä¸¤ä¸ªLeRobotæ•°æ®é›†ä¸ºä¸€ä¸ªæ–°çš„æ•°æ®é›†

Example usage:
    python merge_lerobot_datasets.py \
        --dataset1 /path/to/dataset1 \
        --dataset2 /path/to/dataset2 \
        --output /path/to/merged_dataset \
        --output-repo-id merged_dataset_name

åŠŸèƒ½è¯´æ˜ï¼š
- å°†ä¸¤ä¸ªLeRobotæ•°æ®é›†çš„æ‰€æœ‰episodesåˆå¹¶åˆ°ä¸€ä¸ªæ–°çš„æ•°æ®é›†ä¸­
- è‡ªåŠ¨ä¿ç•™æ‰€æœ‰åŸå§‹çš„observationsã€actionså’Œmetadata
- æ”¯æŒè§†é¢‘å’Œå›¾åƒä¸¤ç§æ¨¡å¼
- ä¿æŒåŸå§‹çš„taskæ ‡ç­¾å’Œå…¶ä»–å±æ€§
"""

import argparse
import shutil
from pathlib import Path
from typing import Literal
import os

from lerobot.common.datasets.lerobot_dataset import LeRobotDataset
import tqdm


# è®¾ç½®LeRoboté»˜è®¤è·¯å¾„
if os.getenv("HF_LEROBOT_HOME") is None:
    os.environ["HF_LEROBOT_HOME"] = str(Path.home() / ".cache" / "huggingface" / "lerobot")
LEROBOT_HOME = Path(os.getenv("HF_LEROBOT_HOME"))


def validate_datasets_compatibility(dataset1: LeRobotDataset, dataset2: LeRobotDataset) -> tuple[bool, str]:
    """
    éªŒè¯ä¸¤ä¸ªæ•°æ®é›†æ˜¯å¦å…¼å®¹ï¼Œå¯ä»¥åˆå¹¶
    
    Returns:
        (is_compatible, message): å…¼å®¹æ€§æ£€æŸ¥ç»“æœå’Œæ¶ˆæ¯
    """
    # æ£€æŸ¥FPSæ˜¯å¦ä¸€è‡´
    if dataset1.fps != dataset2.fps:
        return False, f"FPSä¸åŒ¹é…: dataset1={dataset1.fps}, dataset2={dataset2.fps}"
    
    # æ£€æŸ¥robot_typeæ˜¯å¦ä¸€è‡´
    if dataset1.meta.robot_type != dataset2.meta.robot_type:
        return False, f"æœºå™¨äººç±»å‹ä¸åŒ¹é…: dataset1={dataset1.meta.robot_type}, dataset2={dataset2.meta.robot_type}"
    
    # æ£€æŸ¥ç‰¹å¾æ˜¯å¦ä¸€è‡´
    features1 = set(dataset1.hf_dataset.column_names)
    features2 = set(dataset2.hf_dataset.column_names)
    
    if features1 != features2:
        missing_in_1 = features2 - features1
        missing_in_2 = features1 - features2
        msg = "ç‰¹å¾ä¸åŒ¹é…:\n"
        if missing_in_1:
            msg += f"  dataset1ç¼ºå°‘: {missing_in_1}\n"
        if missing_in_2:
            msg += f"  dataset2ç¼ºå°‘: {missing_in_2}\n"
        return False, msg
    
    # æ£€æŸ¥æ¯ä¸ªç‰¹å¾çš„shapeæ˜¯å¦ä¸€è‡´ï¼ˆæ’é™¤batchç»´åº¦ï¼‰
    for feature in features1:
        if feature in ["index", "episode_index", "frame_index", "timestamp", "task_index"]:
            continue  # è¿™äº›æ˜¯metadataï¼Œä¸éœ€è¦æ£€æŸ¥shape
        
        try:
            sample1 = dataset1.hf_dataset[0][feature]
            sample2 = dataset2.hf_dataset[0][feature]
            
            if hasattr(sample1, 'shape') and hasattr(sample2, 'shape'):
                if sample1.shape != sample2.shape:
                    return False, f"ç‰¹å¾'{feature}'çš„shapeä¸åŒ¹é…: {sample1.shape} vs {sample2.shape}"
        except Exception as e:
            print(f"è­¦å‘Š: æ£€æŸ¥ç‰¹å¾'{feature}'æ—¶å‡ºé”™: {e}")
    
    return True, "æ•°æ®é›†å…¼å®¹"


def create_merged_dataset(
    dataset1: LeRobotDataset,
    dataset2: LeRobotDataset,
    output_repo_id: str,
    mode: Literal["video", "image"] = "video",
) -> LeRobotDataset:
    """
    åˆ›å»ºä¸€ä¸ªæ–°çš„ç©ºæ•°æ®é›†ï¼Œé…ç½®ä¸æºæ•°æ®é›†ä¸€è‡´
    """
    # è·å–featuresé…ç½®ï¼ˆä»dataset1ï¼‰
    features = {}
    
    # è·å–æ‰€æœ‰åˆ—å
    column_names = dataset1.hf_dataset.column_names
    
    # æ’é™¤å…ƒæ•°æ®åˆ—
    metadata_cols = ["index", "episode_index", "frame_index", "timestamp", "task_index"]
    
    for col in column_names:
        if col in metadata_cols:
            continue
        
        # è·å–ç¬¬ä¸€å¸§çš„æ ·æœ¬
        sample = dataset1.hf_dataset[0][col]
        
        if hasattr(sample, 'shape') and hasattr(sample, 'dtype'):
            # å¤„ç†å›¾åƒç‰¹å¾
            if "images" in col:
                features[col] = {
                    "dtype": mode,
                    "shape": tuple(sample.shape),
                    "names": ["channels", "height", "width"],
                }
            else:
                # å¤„ç†å…¶ä»–å¼ é‡ç‰¹å¾ï¼ˆstate, action, velocity, effortç­‰ï¼‰
                features[col] = {
                    "dtype": str(sample.dtype).replace("torch.", ""),
                    "shape": tuple(sample.shape),
                    "names": None,
                }
    
    # åˆ é™¤å·²å­˜åœ¨çš„è¾“å‡ºç›®å½•
    output_path = LEROBOT_HOME / output_repo_id
    if output_path.exists():
        print(f"âš ï¸  åˆ é™¤å·²å­˜åœ¨çš„è¾“å‡ºç›®å½•: {output_path}")
        shutil.rmtree(output_path)
    
    # åˆ›å»ºæ–°æ•°æ®é›†
    print(f"ğŸ”¨ åˆ›å»ºæ–°æ•°æ®é›†: {output_repo_id}")
    merged_dataset = LeRobotDataset.create(
        repo_id=output_repo_id,
        fps=dataset1.fps,
        robot_type=dataset1.meta.robot_type,
        features=features,
        use_videos=(mode == "video"),
    )
    
    return merged_dataset


def copy_episode_to_dataset(
    source_dataset: LeRobotDataset,
    target_dataset: LeRobotDataset,
    episode_idx: int,
) -> int:
    """
    å°†æºæ•°æ®é›†çš„æŒ‡å®šepisodeå¤åˆ¶åˆ°ç›®æ ‡æ•°æ®é›†
    
    Returns:
        å¤åˆ¶çš„å¸§æ•°
    """
    # è·å–episodeçš„å¸§ç´¢å¼•èŒƒå›´
    episode_data_index = source_dataset.episode_data_index
    from_idx = episode_data_index["from"][episode_idx].item()
    to_idx = episode_data_index["to"][episode_idx].item()
    
    # å¤åˆ¶æ¯ä¸€å¸§
    for frame_idx in range(from_idx, to_idx):
        frame_data = {}
        
        # è·å–æ‰€æœ‰ç‰¹å¾
        for col in source_dataset.hf_dataset.column_names:
            if col in ["index", "episode_index", "frame_index", "timestamp"]:
                continue  # è¿™äº›ç”±target_datasetè‡ªåŠ¨ç”Ÿæˆ
            
            # task_indexéœ€è¦è½¬æ¢ä¸ºtaskæ–‡æœ¬
            if col == "task_index":
                task_index = source_dataset.hf_dataset[frame_idx][col]
                # ä»meta.tasksä¸­è·å–å®é™…çš„taskæ–‡æœ¬
                task_text = source_dataset.meta.tasks.get(int(task_index))
                if task_text is not None:
                    # æ·»åŠ taskå­—æ®µï¼Œsave_episodeä¼šè‡ªåŠ¨è½¬æ¢ä¸ºtask_index
                    frame_data["task"] = task_text
                continue
            
            value = source_dataset.hf_dataset[frame_idx][col]
            frame_data[col] = value
        
        target_dataset.add_frame(frame_data)
    
    # ä¿å­˜episode
    target_dataset.save_episode()
    
    return to_idx - from_idx


def merge_datasets(
    dataset1_path: str,
    dataset2_path: str,
    output_repo_id: str,
    mode: Literal["video", "image"] = "video",
) -> LeRobotDataset:
    """
    åˆå¹¶ä¸¤ä¸ªLeRobotæ•°æ®é›†
    
    Args:
        dataset1_path: ç¬¬ä¸€ä¸ªæ•°æ®é›†è·¯å¾„
        dataset2_path: ç¬¬äºŒä¸ªæ•°æ®é›†è·¯å¾„
        output_repo_id: è¾“å‡ºæ•°æ®é›†çš„repo_id
        mode: 'video' æˆ– 'image' æ¨¡å¼
    
    Returns:
        åˆå¹¶åçš„æ•°æ®é›†
    """
    print("="*80)
    print("ğŸ“¦ åŠ è½½æ•°æ®é›†...")
    print("="*80)
    
    # åŠ è½½ä¸¤ä¸ªæ•°æ®é›†
    print(f"åŠ è½½æ•°æ®é›†1: {dataset1_path}")
    dataset1 = LeRobotDataset(dataset1_path)
    print(f"  âœ… å·²åŠ è½½ {dataset1.num_episodes} episodes, {len(dataset1)} å¸§")
    
    print(f"\nåŠ è½½æ•°æ®é›†2: {dataset2_path}")
    dataset2 = LeRobotDataset(dataset2_path)
    print(f"  âœ… å·²åŠ è½½ {dataset2.num_episodes} episodes, {len(dataset2)} å¸§")
    
    # éªŒè¯å…¼å®¹æ€§
    print("\n" + "="*80)
    print("ğŸ” éªŒè¯æ•°æ®é›†å…¼å®¹æ€§...")
    print("="*80)
    
    is_compatible, message = validate_datasets_compatibility(dataset1, dataset2)
    if not is_compatible:
        raise ValueError(f"æ•°æ®é›†ä¸å…¼å®¹: {message}")
    print(f"  âœ… {message}")
    
    # åˆ›å»ºåˆå¹¶åçš„æ•°æ®é›†
    print("\n" + "="*80)
    print("ğŸ”¨ åˆ›å»ºåˆå¹¶æ•°æ®é›†...")
    print("="*80)
    
    merged_dataset = create_merged_dataset(dataset1, dataset2, output_repo_id, mode)
    
    # å¤åˆ¶dataset1çš„æ‰€æœ‰episodes
    print("\n" + "="*80)
    print(f"ğŸ“‹ å¤åˆ¶æ•°æ®é›†1çš„episodes...")
    print("="*80)
    
    total_frames = 0
    for ep_idx in tqdm.tqdm(range(dataset1.num_episodes), desc="Dataset1"):
        num_frames = copy_episode_to_dataset(dataset1, merged_dataset, ep_idx)
        total_frames += num_frames
    
    print(f"  âœ… å·²å¤åˆ¶ {dataset1.num_episodes} episodes, {total_frames} å¸§")
    
    # å¤åˆ¶dataset2çš„æ‰€æœ‰episodes
    print("\n" + "="*80)
    print(f"ğŸ“‹ å¤åˆ¶æ•°æ®é›†2çš„episodes...")
    print("="*80)
    
    total_frames = 0
    for ep_idx in tqdm.tqdm(range(dataset2.num_episodes), desc="Dataset2"):
        num_frames = copy_episode_to_dataset(dataset2, merged_dataset, ep_idx)
        total_frames += num_frames
    
    print(f"  âœ… å·²å¤åˆ¶ {dataset2.num_episodes} episodes, {total_frames} å¸§")
    
    # æ‰“å°åˆå¹¶ç»“æœ
    print("\n" + "="*80)
    print("âœ¨ åˆå¹¶å®Œæˆï¼")
    print("="*80)
    print(f"æ€»Episodesæ•°: {merged_dataset.num_episodes}")
    print(f"  - æ¥è‡ªæ•°æ®é›†1: {dataset1.num_episodes}")
    print(f"  - æ¥è‡ªæ•°æ®é›†2: {dataset2.num_episodes}")
    print(f"æ€»å¸§æ•°: {len(merged_dataset)}")
    print(f"  - æ¥è‡ªæ•°æ®é›†1: {len(dataset1)}")
    print(f"  - æ¥è‡ªæ•°æ®é›†2: {len(dataset2)}")
    print(f"ä¿å­˜ä½ç½®: {LEROBOT_HOME / output_repo_id}")
    print("="*80)
    
    return merged_dataset


def main():
    parser = argparse.ArgumentParser(
        description="åˆå¹¶ä¸¤ä¸ªLeRobotæ•°æ®é›†",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # åˆå¹¶ä¸¤ä¸ªæ•°æ®é›†ï¼ˆvideoæ¨¡å¼ï¼‰
  python merge_lerobot_datasets.py \\
      --dataset1 /path/to/dataset1 \\
      --dataset2 /path/to/dataset2 \\
      --output-repo-id merged_dataset \\
      --mode video
  
  # åˆå¹¶ä¸¤ä¸ªæ•°æ®é›†ï¼ˆimageæ¨¡å¼ï¼‰
  python merge_lerobot_datasets.py \\
      --dataset1 /path/to/dataset1 \\
      --dataset2 /path/to/dataset2 \\
      --output-repo-id merged_dataset \\
      --mode image
        """
    )
    
    parser.add_argument(
        "--dataset1",
        type=str,
        required=True,
        help="ç¬¬ä¸€ä¸ªæ•°æ®é›†çš„è·¯å¾„"
    )
    
    parser.add_argument(
        "--dataset2",
        type=str,
        required=True,
        help="ç¬¬äºŒä¸ªæ•°æ®é›†çš„è·¯å¾„"
    )
    
    parser.add_argument(
        "--output-repo-id",
        type=str,
        required=True,
        help="è¾“å‡ºæ•°æ®é›†çš„repo_idï¼ˆåç§°ï¼‰"
    )
    
    parser.add_argument(
        "--mode",
        type=str,
        default="video",
        choices=["video", "image"],
        help="æ•°æ®é›†æ¨¡å¼: videoï¼ˆä½¿ç”¨è§†é¢‘å‹ç¼©ï¼‰æˆ– imageï¼ˆä½¿ç”¨ç‹¬ç«‹å›¾åƒï¼‰"
    )
    
    parser.add_argument(
        "--consolidate",
        action="store_true",
        help="åˆå¹¶åæ˜¯å¦consolidateæ•°æ®é›†ï¼ˆå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼‰"
    )
    
    args = parser.parse_args()
    
    # æ‰§è¡Œåˆå¹¶
    try:
        merged_dataset = merge_datasets(
            args.dataset1,
            args.dataset2,
            args.output_repo_id,
            args.mode,
        )
        
        # å¯é€‰çš„consolidateæ­¥éª¤
        if args.consolidate:
            print("\n" + "="*80)
            print("ğŸ”„ Consolidatingæ•°æ®é›†...")
            print("="*80)
            merged_dataset.consolidate()
            print("  âœ… Consolidateå®Œæˆ")
        
        print("\nâœ… æ‰€æœ‰æ“ä½œå®Œæˆï¼")
        
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
